{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c3e376c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import string\n",
    "import pickle\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import wget\n",
    "import zipfile\n",
    "import json5  \n",
    "from safetensors.torch import load_file, save_file \n",
    "from typing import Tuple, List, Dict, Any, Union\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e539912b",
   "metadata": {},
   "source": [
    "**Setup and Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87224d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingConfig:\n",
    "    def __init__(self, config_path: str = 'config.json5'):\n",
    "        print(f\"Loading configuration from: {config_path}\")\n",
    "        with open(config_path, 'r') as f:\n",
    "            config_data = json5.load(f)\n",
    "\n",
    "        for key, value in config_data.items():\n",
    "            setattr(self, key, value)\n",
    "        \n",
    "        self.__post_init__()\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"Initializes computed fields and directories after loading.\"\"\"\n",
    "        if isinstance(self.antenna_indices, int):\n",
    "            self.antenna_indices = [self.antenna_indices]\n",
    "        \n",
    "        if not self.antenna_indices:\n",
    "            self.antenna_indices = list(range(4))\n",
    "            print(\"WARNING: 'antenna_indices' not specified, using all 4 antennas.\")\n",
    "\n",
    "        self.input_channels = len(self.antenna_indices)\n",
    "        \n",
    "        if self.input_channels == 1:\n",
    "            run_name_prefix = f'single_antenna_{self.antenna_indices[0]}'\n",
    "        elif self.input_channels == 4:\n",
    "            run_name_prefix = 'all_antennas'\n",
    "        else:\n",
    "            antennas_str = '_'.join(map(str, sorted(self.antenna_indices)))\n",
    "            run_name_prefix = f'custom_antennas_{antennas_str}'\n",
    "            \n",
    "        self.current_run_name = f'{run_name_prefix}'\n",
    "\n",
    "        self.current_checkpoint_dir = os.path.join(self.checkpoint_dir_base, self.current_run_name)\n",
    "        self.current_log_dir = os.path.join(self.log_dir_base, self.current_run_name)\n",
    "        self.dataset_path = os.path.join(self.base_data_dir, self.dataset_name, \"dataset\")\n",
    "\n",
    "        os.makedirs(self.current_checkpoint_dir, exist_ok=True)\n",
    "        os.makedirs(self.current_log_dir, exist_ok=True)\n",
    "        os.makedirs(self.dataset_path, exist_ok=True)\n",
    "\n",
    "        final_decoder_out_channels = self.input_channels\n",
    "        self.decoder_conv_transpose_configs.append(\n",
    "            {\"out_channels\": final_decoder_out_channels, \"kernel_size\": (1, 1), \"stride\": (1, 1), \"padding\": 0, \"activation\": \"Sigmoid\"}\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def get_activation(activation_name: str) -> nn.Module:\n",
    "        \n",
    "        if activation_name is None:\n",
    "            return nn.Identity()\n",
    "\n",
    "        try:\n",
    "            activation_class = getattr(nn, activation_name)\n",
    "            return activation_class()\n",
    "        except AttributeError:\n",
    "            raise ValueError(f\"Activation function not found in torch: '{activation_name}'\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"TrainingConfig(run_name='{self.current_run_name}', device='{self.device}')\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ebd976",
   "metadata": {},
   "source": [
    "**Data Handling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d89eb983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_prepare_data(config: TrainingConfig):\n",
    "    zip_path = os.path.join(config.base_data_dir, f\"{config.dataset_name}.zip\")\n",
    "    \n",
    "    example_mat_file = os.path.join(config.dataset_path, f\"{config.dataset_name}a_A.mat\")\n",
    "    if os.path.exists(example_mat_file):\n",
    "        print(f\"Data already found in {config.dataset_path}\")\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(zip_path):\n",
    "        print(f\"Downloading data from {config.raw_data_zip_url}...\")\n",
    "        wget.download(config.raw_data_zip_url, zip_path)\n",
    "        print(\"Download complete.\")\n",
    "    else:\n",
    "        print(f\"ZIP file found: {zip_path}\")\n",
    "\n",
    "    print(f\"Extracting data to {os.path.join(config.base_data_dir, config.dataset_name)}...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(os.path.join(config.base_data_dir, config.dataset_name))\n",
    "    print(\"Extraction complete.\")\n",
    "\n",
    "\n",
    "\n",
    "class CsiPyTorchDataset(Dataset):\n",
    "    def __init__(self, config: TrainingConfig, file_list: List[str]):\n",
    "        self.config = config\n",
    "        self.window_size = config.window_size\n",
    "        self.samples_per_file = config.samples_per_file\n",
    "        self.antenna_indices = config.antenna_indices\n",
    "        self.input_channels = config.input_channels\n",
    "\n",
    "        self.all_csi_segments = []\n",
    "        self.all_labels_for_windows = []\n",
    "        self.all_start_indices_in_concatenated_csi = []\n",
    "\n",
    "        current_concat_offset = 0\n",
    "        print(\"Loading MAT files...\")\n",
    "        for activity_idx, file_path in enumerate(file_list):\n",
    "            try:\n",
    "                mat = sio.loadmat(file_path)\n",
    "                data = np.array(mat['csi'])  # Shape (raw_samples, features, num_antennas_in_file)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file_path}: {e}. Skipped.\")\n",
    "                continue\n",
    "\n",
    "            num_raw_samples_in_file = data.shape[0]\n",
    "            samples_to_take = min(self.samples_per_file, num_raw_samples_in_file)\n",
    "            \n",
    "            # --- Antenna selection logic ---\n",
    "            max_antenna_idx = max(self.antenna_indices)\n",
    "            if data.ndim < 3 or data.shape[2] <= max_antenna_idx:\n",
    "                raise ValueError(f\"Data in {file_path} is not compatible. Required antenna index {max_antenna_idx}, \"\n",
    "                                 f\"but data shape is {data.shape}.\")\n",
    "\n",
    "            # Select the specified antennas and maintain the shape for subsequent processing\n",
    "            selected_data = data[:samples_to_take, :, self.antenna_indices]\n",
    "            \n",
    "            # If only one antenna is selected, np.squeeze might remove the dimension. We restore it.\n",
    "            if selected_data.ndim == 2:\n",
    "                selected_data = np.expand_dims(selected_data, axis=2)\n",
    "\n",
    "            data = np.round(np.abs(selected_data)).astype(np.float32)\n",
    "            self.all_csi_segments.append(torch.from_numpy(data))\n",
    "\n",
    "            num_possible_windows_this_file = data.shape[0] - self.window_size + 1\n",
    "            if num_possible_windows_this_file <= 0:\n",
    "                print(f\"Warning: samples_per_file ({data.shape[0]}) in {file_path} Ã¨ minore di window_size ({self.window_size}).\")\n",
    "                continue\n",
    "            \n",
    "            for i in range(num_possible_windows_this_file):\n",
    "                self.all_start_indices_in_concatenated_csi.append(current_concat_offset + i)\n",
    "                self.all_labels_for_windows.append(activity_idx)\n",
    "\n",
    "            current_concat_offset += data.shape[0]\n",
    "            print(f\"Processed  {file_path}, {num_possible_windows_this_file} windows added.\")\n",
    "\n",
    "        if not self.all_csi_segments:\n",
    "            raise RuntimeError(\"No CSI data loaded. Check paths and MAT files.\")\n",
    "\n",
    "        self.csi_data_concatenated = torch.cat(self.all_csi_segments, dim=0)\n",
    "\n",
    "        # Normalization of CSI data\n",
    "        if self.csi_data_concatenated.numel() > 0:\n",
    "            max_val = torch.max(self.csi_data_concatenated)\n",
    "            if max_val > 0:\n",
    "                self.csi_data_concatenated /= max_val\n",
    "        \n",
    "        print(f\"Dataset initialized. CSI shape: {self.csi_data_concatenated.shape}\")\n",
    "        print(f\"Total number of windows: {len(self)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_start_indices_in_concatenated_csi)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        actual_start_idx = self.all_start_indices_in_concatenated_csi[idx]\n",
    "        window_data = self.csi_data_concatenated[actual_start_idx : actual_start_idx + self.window_size, ...]\n",
    "        \n",
    "         # PyTorch Conv2d expects (Batch, Channels, Height, Width)\n",
    "        # Here: Channels = num_antennas, Height = window_size, Width = features\n",
    "        # Permute from (window_size, features, channels) to (channels, window_size, features)\n",
    "        \n",
    "        window_data = window_data.permute(2, 0, 1)\n",
    "\n",
    "        label = self.all_labels_for_windows[idx]\n",
    "        return window_data, torch.tensor(label, dtype=torch.long)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cccc0f6",
   "metadata": {},
   "source": [
    "**Variational Autoencoder (VAE) Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c10dc4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        layers = []\n",
    "        current_channels = config.input_channels\n",
    "        dummy_h, dummy_w = config.window_size, config.feature_dim \n",
    "        \n",
    "        for layer_cfg in config.encoder_conv_configs:\n",
    "            # Extract parameters for Conv2d, excluding 'activation'\n",
    "            conv_params = {k: v for k, v in layer_cfg.items() if k != 'activation'}\n",
    "            layers.append(nn.Conv2d(current_channels, **conv_params))\n",
    "            current_channels = layer_cfg[\"out_channels\"]\n",
    "            if \"activation\" in layer_cfg:\n",
    "                # Call the static method from the TrainingConfig class\n",
    "                layers.append(TrainingConfig.get_activation(layer_cfg[\"activation\"]))\n",
    "            \n",
    "            kh, kw = layer_cfg[\"kernel_size\"]\n",
    "            sh, sw = layer_cfg[\"stride\"]\n",
    "            ph, pw = (layer_cfg.get(\"padding\", 0),)*2 if isinstance(layer_cfg.get(\"padding\", 0), int) else layer_cfg.get(\"padding\", 0)\n",
    "            dummy_h = math.floor((dummy_h + 2 * ph - kh) / sh + 1)\n",
    "            dummy_w = math.floor((dummy_w + 2 * pw - kw) / sw + 1)\n",
    "\n",
    "        layers.append(nn.Flatten())\n",
    "        self.conv_to_flatten_shape = (current_channels, dummy_h, dummy_w)\n",
    "        flattened_size = current_channels * dummy_h * dummy_w\n",
    "        \n",
    "        current_features = flattened_size \n",
    "        for layer_cfg in config.encoder_fc_configs:\n",
    "            layers.append(nn.Linear(current_features, layer_cfg[\"out_features\"]))\n",
    "            current_features = layer_cfg[\"out_features\"]\n",
    "            if \"activation\" in layer_cfg:\n",
    "                layers.append(TrainingConfig.get_activation(layer_cfg[\"activation\"]))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.fc_z_mean = nn.Linear(current_features, config.latent_dim)\n",
    "        self.fc_z_log_var = nn.Linear(current_features, config.latent_dim)\n",
    "    \n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        x_encoded = self.model(x)\n",
    "        z_mean = self.fc_z_mean(x_encoded)\n",
    "        z_log_var = self.fc_z_log_var(x_encoded)\n",
    "        std = torch.exp(0.5 * z_log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = z_mean + eps * std\n",
    "        return z_mean, z_log_var, z\n",
    "\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, config: TrainingConfig, encoder_conv_output_shape: Tuple[int, int, int]):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.encoder_conv_output_shape = encoder_conv_output_shape\n",
    "        layers = []\n",
    "        decoder_start_features = np.prod(encoder_conv_output_shape)\n",
    "        \n",
    "        current_features = config.latent_dim\n",
    "        if config.decoder_fc_configs:\n",
    "            for layer_cfg in config.decoder_fc_configs:\n",
    "                layers.append(nn.Linear(current_features, layer_cfg[\"out_features\"]))\n",
    "                current_features = layer_cfg[\"out_features\"]\n",
    "                if \"activation\" in layer_cfg:\n",
    "                    layers.append(TrainingConfig.get_activation(layer_cfg[\"activation\"]))\n",
    "            layers.append(nn.Linear(current_features, decoder_start_features))\n",
    "        else:\n",
    "            layers.append(nn.Linear(config.latent_dim, decoder_start_features))\n",
    "\n",
    "        if config.encoder_fc_configs:\n",
    "             layers.append(TrainingConfig.get_activation(config.encoder_fc_configs[-1]['activation']))\n",
    "\n",
    "        self.fc_part = nn.Sequential(*layers)\n",
    "        \n",
    "        conv_transpose_layers = []\n",
    "        current_channels = self.encoder_conv_output_shape[0]\n",
    "        for i, layer_cfg in enumerate(config.decoder_conv_transpose_configs):\n",
    "            conv_params = {k: v for k, v in layer_cfg.items() if k != 'activation'}\n",
    "            conv_transpose_layers.append(nn.ConvTranspose2d(current_channels, **conv_params))\n",
    "            current_channels = layer_cfg[\"out_channels\"]\n",
    "            if \"activation\" in layer_cfg:\n",
    "                 conv_transpose_layers.append(TrainingConfig.get_activation(layer_cfg[\"activation\"]))\n",
    "\n",
    "        self.conv_transpose_part = nn.Sequential(*conv_transpose_layers)\n",
    "        \n",
    "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.fc_part(z)\n",
    "        x = x.view(-1, *self.encoder_conv_output_shape)\n",
    "        x_reconstructed = self.conv_transpose_part(x)\n",
    "        return x_reconstructed\n",
    "\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(config)\n",
    "        self.decoder = Decoder(config, self.encoder.conv_to_flatten_shape)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        z_mean, z_log_var, z = self.encoder(x)\n",
    "        x_reconstructed = self.decoder(z)\n",
    "        return x_reconstructed, z_mean, z_log_var, z\n",
    "\n",
    "    def loss_function(self, x_original: torch.Tensor, x_reconstructed: torch.Tensor, \n",
    "                      z_mean: torch.Tensor, z_log_var: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        bce_loss = nn.functional.binary_cross_entropy(x_reconstructed, x_original, reduction='none')\n",
    "        reconstruction_loss = torch.mean(torch.sum(bce_loss, dim=(1, 2, 3)))\n",
    "        kl_loss = torch.mean(-0.5 * torch.sum(1 + z_log_var - z_mean.pow(2) - z_log_var.exp(), dim=1))\n",
    "        total_loss = reconstruction_loss + kl_loss\n",
    "        return {\"total_loss\": total_loss, \"reconstruction_loss\": reconstruction_loss, \"kl_loss\": kl_loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8749da0e",
   "metadata": {},
   "source": [
    "**Training and Latent Space Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c24d5633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae(config: TrainingConfig, vae: VAE, train_loader: DataLoader):\n",
    "    device = torch.device(config.device)\n",
    "    vae.to(device)\n",
    "    optimizer = optim.Adam(vae.parameters(), lr=config.learning_rate)\n",
    "\n",
    "    start_epoch = 0\n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    log_path = os.path.join(config.current_log_dir, \"training_log.csv\")\n",
    "    if os.path.exists(log_path) and config.load_pretrained_if_exists:\n",
    "        log_df = pd.read_csv(log_path)\n",
    "        if not log_df.empty:\n",
    "            start_epoch = log_df[\"epoch\"].iloc[-1] + 1\n",
    "            best_loss = log_df[config.early_stopping_monitor].min()\n",
    "    else:\n",
    "        log_df = pd.DataFrame(columns=[\"epoch\", \"total_loss\", \"reconstruction_loss\", \"kl_loss\"])\n",
    "\n",
    "    if config.load_pretrained_if_exists:\n",
    "        checkpoint_file = os.path.join(config.current_checkpoint_dir, \"vae_best.safetensors\")\n",
    "        if os.path.exists(checkpoint_file):\n",
    "            print(f\"CLoading model from {checkpoint_file}\")\n",
    "            load_file(vae, checkpoint_file, device=str(device))\n",
    "            print(f\"Model loaded. Resuming from epoch {start_epoch}.\")\n",
    "        else:\n",
    "            print(f\"No safetensors checkpoint found. Training from scratch.\")\n",
    "\n",
    "    for epoch in range(start_epoch, config.epochs):\n",
    "        vae.train()\n",
    "        epoch_losses = {\"total_loss\": 0, \"reconstruction_loss\": 0, \"kl_loss\": 0}\n",
    "        \n",
    "        for data, _ in train_loader:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            x_reconstructed, z_mean, z_log_var, _ = vae(data)\n",
    "            losses = vae.loss_function(data, x_reconstructed, z_mean, z_log_var)\n",
    "            losses[\"total_loss\"].backward()\n",
    "            optimizer.step()\n",
    "            for k, v in losses.items():\n",
    "                epoch_losses[k] += v.item()\n",
    "\n",
    "        avg_losses = {k: v / len(train_loader) for k, v in epoch_losses.items()}\n",
    "        print(f\"--- Epoch {epoch+1} | Avg Loss: {avg_losses['total_loss']:.4f} ---\")\n",
    "\n",
    "        new_log_row = pd.DataFrame([{\"epoch\": epoch, **avg_losses}])\n",
    "        log_df = pd.concat([log_df, new_log_row], ignore_index=True)\n",
    "        log_df.to_csv(log_path, index=False)\n",
    "\n",
    "        current_loss = avg_losses[config.early_stopping_monitor]\n",
    "        if current_loss < best_loss - config.early_stopping_min_delta:\n",
    "            best_loss = current_loss\n",
    "            epochs_no_improve = 0\n",
    "            save_file(vae.state_dict(), os.path.join(config.current_checkpoint_dir, \"vae_best.safetensors\"))\n",
    "            print(f\"New best loss: {best_loss:.4f}. Checkpoint saved as vae_best.safetensors\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        if epoch > 0 and epoch % config.save_every_n_epochs == 0:\n",
    "            save_file(vae.state_dict(), os.path.join(config.current_checkpoint_dir, f\"vae_epoch_{epoch}.safetensors\"))\n",
    "            print(f\"Checkpoint saved for epoch {epoch} as  .safetensors\")\n",
    "\n",
    "        if epochs_no_improve >= config.early_stopping_patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "            break\n",
    "            \n",
    "    save_file(vae.state_dict(), os.path.join(config.current_checkpoint_dir, \"vae_final.safetensors\"))\n",
    "    print(\"Training complete. Final model saved as vae_final.safetensors\")\n",
    "\n",
    "\n",
    "\n",
    "def generate_and_save_latent_space(config: TrainingConfig, vae: VAE, data_loader: DataLoader):\n",
    "    device = torch.device(config.device)\n",
    "    vae.to(device)\n",
    "    vae.eval()\n",
    "\n",
    "    checkpoint_path = os.path.join(config.current_checkpoint_dir, \"vae_best.safetensors\")\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        checkpoint_path = os.path.join(config.current_checkpoint_dir, \"vae_final.safetensors\")\n",
    "    \n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"Loading model for inference from {checkpoint_path}\")\n",
    "        load_file(vae, checkpoint_path, device=str(device))\n",
    "    else:\n",
    "        print(\"WARNING: No safetensors model found for inference.\")\n",
    "\n",
    "    all_z_mean, all_z_log_var, all_labels = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for data, labels in data_loader:\n",
    "            data = data.to(device)\n",
    "            z_mean, z_log_var, _ = vae.encoder(data)\n",
    "            all_z_mean.append(z_mean.cpu().numpy())\n",
    "            all_z_log_var.append(z_log_var.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    z_data_combined = np.concatenate([np.concatenate(all_z_mean), np.concatenate(all_z_log_var)], axis=1)\n",
    "    labels_np = np.concatenate(all_labels)\n",
    "\n",
    "    latent_space_dir = os.path.join(config.base_data_dir, \"S1/latent_space_dataset\")\n",
    "    os.makedirs(latent_space_dir, exist_ok=True)\n",
    "    \n",
    "    output_filename = os.path.join(latent_space_dir, f\"{config.current_run_name}.pkl\")\n",
    "    with open(output_filename, 'wb') as f:\n",
    "        pickle.dump([z_data_combined, labels_np], f)\n",
    "    print(f\"Latent space data saved to: {output_filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d783503f",
   "metadata": {},
   "source": [
    "**Main Execution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700cf1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento configurazione da: config.json5\n",
      "Configurazione caricata: TrainingConfig(run_name='custom_antennas_1_2', device='cpu')\n",
      "Dati giÃ  trovati in ./data\\S1\\dataset\n",
      "Lista file per il dataset: ['./data\\\\S1\\\\dataset\\\\S1a_A.mat', './data\\\\S1\\\\dataset\\\\S1a_B.mat', './data\\\\S1\\\\dataset\\\\S1a_C.mat', './data\\\\S1\\\\dataset\\\\S1a_D.mat', './data\\\\S1\\\\dataset\\\\S1a_E.mat']\n",
      "Caricamento file MAT...\n",
      "Processato ./data\\S1\\dataset\\S1a_A.mat, 11551 finestre aggiunte.\n",
      "Processato ./data\\S1\\dataset\\S1a_B.mat, 11551 finestre aggiunte.\n",
      "Processato ./data\\S1\\dataset\\S1a_C.mat, 11551 finestre aggiunte.\n",
      "Processato ./data\\S1\\dataset\\S1a_D.mat, 11551 finestre aggiunte.\n",
      "Processato ./data\\S1\\dataset\\S1a_E.mat, 11551 finestre aggiunte.\n",
      "Dataset inizializzato. CSI shape: torch.Size([60000, 2048, 2])\n",
      "Numero totale di finestre: 57755\n",
      "Parametri totali del modello: 185942\n",
      "\n",
      "--- Inizio Addestramento ---\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    config = TrainingConfig(config_path='config.json5')\n",
    "    print(f\"Configuration loaded: {config}\")\n",
    "\n",
    "    download_and_prepare_data(config)\n",
    "    \n",
    "    file_list = [os.path.join(config.dataset_path, f\"{config.dataset_name}a_{x}.mat\")\n",
    "                 for x in string.ascii_uppercase[:config.num_activities]]\n",
    "    print(f\"File list for the dataset: {file_list}\")\n",
    "\n",
    "    try:\n",
    "        csi_dataset = CsiPyTorchDataset(config, file_list)\n",
    "        if len(csi_dataset) == 0:\n",
    "            print(\"ERROR: Dataset is empty. Terminating.\")\n",
    "            exit()\n",
    "        train_loader = DataLoader(csi_dataset, batch_size=config.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"Error during dataset/dataloader creation: {e}\")\n",
    "        traceback.print_exc()\n",
    "        exit()\n",
    "\n",
    "    vae_model = VAE(config)\n",
    "    print(f\"Total model parameters: {sum(p.numel() for p in vae_model.parameters())}\")\n",
    "    \n",
    "    print(\"\\n--- Starting Training ---\")\n",
    "    train_vae(config, vae_model, train_loader)\n",
    "\n",
    "    print(\"\\n--- Starting Latent Space Generation ---\")\n",
    "    generate_and_save_latent_space(config, vae_model, train_loader)\n",
    "\n",
    "    print(\"\\nProcess completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
