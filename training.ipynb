{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ca69936",
   "metadata": {},
   "source": [
    "<h1> Training </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "829e5b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import string\n",
    "import pickle\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import wget\n",
    "import zipfile\n",
    "import json5  \n",
    "from safetensors.torch import load_file, save_file \n",
    "from typing import Tuple, List, Dict\n",
    "import pandas as pd\n",
    "import import_ipynb\n",
    "\n",
    "\n",
    "\n",
    "from config_setup import TrainingConfig\n",
    "from model import VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ec24c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae(config: TrainingConfig, vae: VAE, train_loader: DataLoader):\n",
    "    \n",
    "    device = torch.device(config.device)\n",
    "    vae.to(device)\n",
    "    optimizer = optim.Adam(vae.parameters(), lr=config.learning_rate)\n",
    "\n",
    "    start_epoch = 0\n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    log_path = os.path.join(config.current_log_dir, \"training_log.csv\")\n",
    "    if os.path.exists(log_path) and config.load_pretrained_if_exists:\n",
    "        log_df = pd.read_csv(log_path)\n",
    "        if not log_df.empty:\n",
    "            start_epoch = log_df[\"epoch\"].iloc[-1] + 1\n",
    "            best_loss = log_df[config.early_stopping_monitor].min()\n",
    "    else:\n",
    "        log_df = pd.DataFrame(columns=[\"epoch\", \"total_loss\", \"reconstruction_loss\", \"kl_loss\"])\n",
    "\n",
    "    if config.load_pretrained_if_exists:\n",
    "        checkpoint_file = os.path.join(config.current_checkpoint_dir, \"vae_best.safetensors\")\n",
    "        if os.path.exists(checkpoint_file):\n",
    "            print(f\"Loading model from {checkpoint_file}\")\n",
    "            \n",
    "            state_dict = load_file(checkpoint_file, device=str(device))\n",
    "            vae.load_state_dict(state_dict)\n",
    "            print(f\"Model loaded. Resuming from epoch {start_epoch}.\")\n",
    "        else:\n",
    "            print(f\"No safetensors checkpoint found. Training from scratch.\")\n",
    "\n",
    "    for epoch in range(start_epoch, config.epochs):\n",
    "        \n",
    "        current_tau = 1.0\n",
    "\n",
    "        if config.latent_distribution == 'gumbel':\n",
    "            anneal_params = config.gumbel_annealing\n",
    "            initial_tau = anneal_params['initial_temperature']\n",
    "            min_tau = anneal_params['min_temperature']\n",
    "            rate = anneal_params['annealing_rate']\n",
    "\n",
    "            current_tau = max(initial_tau * math.exp(-rate * epoch), min_tau) \n",
    "            print(f\"Epoch {epoch+1} | Gumbel Temperature (tau): {current_tau:.4f}\")\n",
    "\n",
    "        \n",
    "        vae.train()\n",
    "        epoch_losses = {\"total_loss\": 0, \"reconstruction_loss\": 0, \"kl_loss\": 0}\n",
    "        \n",
    "        for data, _ in train_loader:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            \n",
    "            model_output = vae(data, temperature=current_tau)\n",
    "            \n",
    "            \n",
    "            losses = vae.loss_function(data, model_output)\n",
    "            \n",
    "            losses[\"total_loss\"].backward()\n",
    "            optimizer.step()\n",
    "            for k, v in losses.items():\n",
    "                epoch_losses[k] += v.item()\n",
    "\n",
    "        avg_losses = {k: v / len(train_loader) for k, v in epoch_losses.items()}\n",
    "        print(f\"--- Epoch {epoch+1} | Avg Loss: {avg_losses['total_loss']:.4f} ---\")\n",
    "\n",
    "        new_log_row = pd.DataFrame([{\"epoch\": epoch, **avg_losses}])\n",
    "        log_df = pd.concat([log_df, new_log_row], ignore_index=True)\n",
    "        log_df.to_csv(log_path, index=False)\n",
    "\n",
    "        current_loss = avg_losses[config.early_stopping_monitor]\n",
    "        if current_loss < best_loss - config.early_stopping_min_delta:\n",
    "            best_loss = current_loss\n",
    "            epochs_no_improve = 0\n",
    "            save_file(vae.state_dict(), os.path.join(config.current_checkpoint_dir, \"vae_best.safetensors\"))\n",
    "            print(f\"New best loss: {best_loss:.4f}. Checkpoint saved as vae_best.safetensors\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        if epoch > 0 and epoch % config.save_every_n_epochs == 0:\n",
    "            # Correzione del nome del file nel messaggio di log\n",
    "            checkpoint_name = f\"vae_epoch_{epoch}.safetensors\"\n",
    "            save_file(vae.state_dict(), os.path.join(config.current_checkpoint_dir, checkpoint_name))\n",
    "            print(f\"Checkpoint saved for epoch {epoch} as {checkpoint_name}\")\n",
    "\n",
    "        if epochs_no_improve >= config.early_stopping_patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "            break\n",
    "            \n",
    "    save_file(vae.state_dict(), os.path.join(config.current_checkpoint_dir, \"vae_final.safetensors\"))\n",
    "    print(\"Training complete. Final model saved as vae_final.safetensors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fc702e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_latent_space(config: TrainingConfig, vae: VAE, train_loader: DataLoader):\n",
    "    device = torch.device(config.device)\n",
    "    vae.to(device)\n",
    "    vae.eval()\n",
    "\n",
    "    dataset= train_loader.dataset\n",
    "\n",
    "    inference_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,  \n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    checkpoint_path = os.path.join(config.current_checkpoint_dir, \"vae_best.safetensors\")\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        checkpoint_path = os.path.join(config.current_checkpoint_dir, \"vae_final.safetensors\")\n",
    "    \n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"Loading model for inference from {checkpoint_path}\")\n",
    "        state_dict = load_file(checkpoint_path, device=str(device))\n",
    "        vae.load_state_dict(state_dict)\n",
    "    else:\n",
    "        print(\"WARNING: No safetensors model found for inference.\")\n",
    "\n",
    "    inference_tau = 1.0\n",
    "    if config.latent_distribution == 'gumbel':\n",
    "        inference_tau = config.gumbel_annealing['min_temperature']\n",
    "\n",
    "    all_labels = []\n",
    "    latent_data_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, labels in inference_loader:\n",
    "            data = data.to(device)\n",
    "            model_output = vae(data, temperature=inference_tau)\n",
    "            \n",
    "            if config.latent_distribution == 'gaussian':\n",
    "                # For the Gaussian VAE, we save the mean as a latent representation\n",
    "                latent_data_list.append(model_output['z_mean'].cpu().numpy())\n",
    "            elif config.latent_distribution == 'gumbel':\n",
    "                # For VAE Gumbel, let's save the one-hot sample\n",
    "                latent_data_list.append(model_output['z'].cpu().numpy())\n",
    "                \n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    z_data_combined = np.concatenate(latent_data_list, axis=0)\n",
    "    labels_np = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    \n",
    "    dist_tag = config.latent_distribution\n",
    "    latent_space_dir_name = f\"latent_space_dataset_{dist_tag}\"\n",
    "    latent_space_dir = os.path.join(config.base_data_dir, latent_space_dir_name)\n",
    "    os.makedirs(latent_space_dir, exist_ok=True)\n",
    "\n",
    "    \n",
    "        \n",
    "    \n",
    "    tensors_to_save = {\n",
    "        \"latent_space\": torch.from_numpy(z_data_combined),\n",
    "        \"labels\": torch.from_numpy(labels_np)\n",
    "    }\n",
    "    \n",
    "\n",
    "    output_filename = os.path.join(latent_space_dir, f\"{config.current_run_name}.safetensors\")\n",
    "    \n",
    "\n",
    "    save_file(tensors_to_save, output_filename)\n",
    "    \n",
    "    print(f\"Latent space data saved to: {output_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
