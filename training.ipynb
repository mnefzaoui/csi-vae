{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ca69936",
   "metadata": {},
   "source": [
    "<h1> Training </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "829e5b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import string\n",
    "import pickle\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import wget\n",
    "import zipfile\n",
    "import json5  \n",
    "from safetensors.torch import load_file, save_file \n",
    "from typing import Tuple, List, Dict\n",
    "import pandas as pd\n",
    "import import_ipynb\n",
    "\n",
    "\n",
    "\n",
    "from config_setup import TrainingConfig\n",
    "from model import VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ec24c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae(config: TrainingConfig, vae: VAE, train_loader: DataLoader):\n",
    "    device = torch.device(config.device)\n",
    "    vae.to(device)\n",
    "    optimizer = optim.Adam(vae.parameters(), lr=config.learning_rate)\n",
    "\n",
    "    start_epoch = 0\n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    log_path = os.path.join(config.current_log_dir, \"training_log.csv\")\n",
    "    if os.path.exists(log_path) and config.load_pretrained_if_exists:\n",
    "        log_df = pd.read_csv(log_path)\n",
    "        if not log_df.empty:\n",
    "            start_epoch = log_df[\"epoch\"].iloc[-1] + 1\n",
    "            best_loss = log_df[config.early_stopping_monitor].min()\n",
    "    else:\n",
    "        log_df = pd.DataFrame(columns=[\"epoch\", \"total_loss\", \"reconstruction_loss\", \"kl_loss\"])\n",
    "\n",
    "    if config.load_pretrained_if_exists:\n",
    "        checkpoint_file = os.path.join(config.current_checkpoint_dir, \"vae_best.safetensors\")\n",
    "        if os.path.exists(checkpoint_file):\n",
    "            print(f\"Loading model from {checkpoint_file}\")\n",
    "            load_file(vae, checkpoint_file, device=str(device))\n",
    "            print(f\"Model loaded. Resuming from epoch {start_epoch}.\")\n",
    "        else:\n",
    "            print(f\"No safetensors checkpoint found. Training from scratch.\")\n",
    "\n",
    "    for epoch in range(start_epoch, config.epochs):\n",
    "        vae.train()\n",
    "        epoch_losses = {\"total_loss\": 0, \"reconstruction_loss\": 0, \"kl_loss\": 0}\n",
    "        \n",
    "        for data, _ in train_loader:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            x_reconstructed, z_mean, z_log_var, _ = vae(data)\n",
    "            losses = vae.loss_function(data, x_reconstructed, z_mean, z_log_var)\n",
    "            losses[\"total_loss\"].backward()\n",
    "            optimizer.step()\n",
    "            for k, v in losses.items():\n",
    "                epoch_losses[k] += v.item()\n",
    "\n",
    "        avg_losses = {k: v / len(train_loader) for k, v in epoch_losses.items()}\n",
    "        print(f\"--- Epoch {epoch+1} | Avg Loss: {avg_losses['total_loss']:.4f} ---\")\n",
    "\n",
    "        new_log_row = pd.DataFrame([{\"epoch\": epoch, **avg_losses}])\n",
    "        log_df = pd.concat([log_df, new_log_row], ignore_index=True)\n",
    "        log_df.to_csv(log_path, index=False)\n",
    "\n",
    "        current_loss = avg_losses[config.early_stopping_monitor]\n",
    "        if current_loss < best_loss - config.early_stopping_min_delta:\n",
    "            best_loss = current_loss\n",
    "            epochs_no_improve = 0\n",
    "            save_file(vae.state_dict(), os.path.join(config.current_checkpoint_dir, \"vae_best.safetensors\"))\n",
    "            print(f\"New best loss: {best_loss:.4f}. Checkpoint saved as vae_best.safetensors\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        if epoch > 0 and epoch % config.save_every_n_epochs == 0:\n",
    "            save_file(vae.state_dict(), os.path.join(config.current_checkpoint_dir, f\"vae_epoch_{epoch}.safetensors\"))\n",
    "            print(f\"Checkpoint saved for epoch {epoch} as  .safetensors\")\n",
    "\n",
    "        if epochs_no_improve >= config.early_stopping_patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "            break\n",
    "            \n",
    "    save_file(vae.state_dict(), os.path.join(config.current_checkpoint_dir, \"vae_final.safetensors\"))\n",
    "    print(\"Training complete. Final model saved as vae_final.safetensors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03b581c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_latent_space(config: TrainingConfig, vae: VAE, data_loader: DataLoader):\n",
    "    device = torch.device(config.device)\n",
    "    vae.to(device)\n",
    "    vae.eval()\n",
    "\n",
    "    checkpoint_path = os.path.join(config.current_checkpoint_dir, \"vae_best.safetensors\")\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        checkpoint_path = os.path.join(config.current_checkpoint_dir, \"vae_final.safetensors\")\n",
    "    \n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"Loading model for inference from {checkpoint_path}\")\n",
    "        load_file(vae, checkpoint_path, device=str(device))\n",
    "    else:\n",
    "        print(\"WARNING: No safetensors model found for inference.\")\n",
    "\n",
    "    all_z_mean, all_z_log_var, all_labels = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for data, labels in data_loader:\n",
    "            data = data.to(device)\n",
    "            z_mean, z_log_var, _ = vae.encoder(data)\n",
    "            all_z_mean.append(z_mean.cpu().numpy())\n",
    "            all_z_log_var.append(z_log_var.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    z_data_combined = np.concatenate([np.concatenate(all_z_mean), np.concatenate(all_z_log_var)], axis=1)\n",
    "    labels_np = np.concatenate(all_labels)\n",
    "\n",
    "    latent_space_dir = os.path.join(config.base_data_dir, \"S1/latent_space_dataset\")\n",
    "    os.makedirs(latent_space_dir, exist_ok=True)\n",
    "    \n",
    "    output_filename = os.path.join(latent_space_dir, f\"{config.current_run_name}.pkl\")\n",
    "    with open(output_filename, 'wb') as f:\n",
    "        pickle.dump([z_data_combined, labels_np], f)\n",
    "    print(f\"Latent space data saved to: {output_filename}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
